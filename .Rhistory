#  if (denoise_data == T) {
#    ans <- denoise(infiles=libs[i],
#                   libtype="454",
#                   outprefix=paste(analysis_dir, '/', "work_lib", i, sep=''))
#  } else {
#    # Checking if provided files are in Fasta format:
#    #ans <- tryCatch(readFastq(paste(analysis_dir, '/', "work_lib", i, ".fasta", sep='')), error=function(e) {e <- -1}, finally=function(ans) {ans <--1})
#
#    #if (ans == -1) {
#    # If so, we just directly copy them into analysis directory:
#    system(paste("cp", libs[i], paste(analysis_dir, '/', "work_lib", i, ".fasta", sep='')))
#    #} else {
#    #  # Otherwise, rase an error and stop:
#    #  print("Error! Provided sequences not in Fasta format!")
#    #}
#  }
#  work_libs[i,1] <- paste(analysis_dir, '/', "work_lib", i, ".fasta", sep='')
#  work_libs[i,2] <- libs[i]
#
#  # Remove chimeras
#  removeChimeras(infile_reads=work_libs[i,1], infile_uchime=paste(work_libs[i,1],".uchime",sep=''))
#
#}
#
#
#
#print(paste("Running clustering application", cluster_app, "...") )
#registerDoMC(length(work_libs)) # Register cores and run in parallel
#foreach(i=1:dim(work_libs)[1]) %dopar%
#  cluster(analysis_dir, default_pref, work_libs[i,1])
work_libs <- matrix(nrow=length(libs), ncol=3)
for (i in seq(libs)) {
work_libs[i,1] <- libs[i] #paste(analysis_dir, '/', "work_lib", i, ".fasta", sep='')
work_libs[i,2] <- libs[i]
}
print(paste("Running clustering application", cluster_app, "...") )
for(i in 1:dim(work_libs)[1]) {
work_libs[i,1] <- cluster2(analysis_dir, default_pref, work_libs[i,1])
}
#==============Read distance matrices==================#
writeMessage("Distance matrix for 16S gene sequences...", logfile, T)
score16S <- generate_distance_matrix16S(ref16S)
writeMessage("Done!", logfile, T)
writeMessage("Distance matrix for guide and amplicon reads...", logfile, T)
scoresV <- readDistanceMatrices(work_libs) # This function reads pre-computed distance matrices sequentially
writeMessage("Done!", logfile, T)
#================Compute MDS for 16S=============#
writeMessage("Computing MDS for 16S...", logfile, T)
mds16S <- read16S(ref16S, score16S ) # Here we are computing a multi-dimensional scaling for 16S gene sequences.
num_ref_points <- dim(mds16S)[1] # Number of reference points.
writeMessage("Done!", logfile, T)
#=============Normalize amplicons=============#
# This is the core of the meta-amplicon analysis algorithm.
writeMessage("Normalization...", logfile, T)
summary_matrix <- normalizeAmplicons(scoresV, mds16S, num_ref_points)
writeMessage("Done!", logfile, T)
#=============Perform final clustering with DBSCAN================#
# Also is an important stage. We have to cluster everything after guide sequences carried back the empirical amplicons.
writeMessage("clustering with DBSCAN...", logfile, T)
tmp_clusters <- clusterDBSCAN(rownames(score16S), summary_matrix,scoresV)
writeMessage("Done!", logfile, T)
temp_clusters
tmp_clusters
scoresV[[2]]
# -*- coding: utf-8 -*-
#
# MetAmp - a software application for meta-amplicon data analysis.
# Written by Ilya Y. Zhbannikov, Feb 2, 2014
# Updated on Sept 7, 2014
# Usage:
# Set the working directory by changing the "dir_path" variable
# Under R-environmentL $>source("main.R")
#
# Set the work directory:
dir_path <- "~/Projects/metamp/" # Working directory where all analysis data will be stored
setwd(dir_path)
# Provide your data here (can be raw or preprocessed libs, program assumes one file for each region):
#libs <- c("Evaluation/data/Dataset1/SRR053817_V1_V3/SRR053817_3.fastq", # V1-3
#          "Evaluation/data/Dataset1/SRR053818_V3_V5/SRR053818_3.fastq", # V3-5
#          "Evaluation/data/Dataset1/SRR053819_V6_V9/SRR053819_3.fastq") # V6-9
libs <- c("Evaluation/data/staggered/SRR072221_forward.fastq", # V1-3
"Evaluation/data/staggered/SRR072237_forward.fastq", # V3-5
"Evaluation/data/staggered/SRR072236_forward.fastq") # V6-9
# Reference sequences:
#ref16S <- "data/LTP/LTP-10271.fasta"
#refs <- c("data/LTP/LTP-10271_V13.fasta", # V1-3
#          "data/LTP/LTP-10271_V35.fasta", # V3-5
#          "data/LTP/LTP-10271_V69.fasta") # V6-9
ref16S <- "Evaluation/data/16S.fasta"
refs <- c("Evaluation/data/V13.fasta", # V1-3
"Evaluation/data/V35.fasta", # V3-5
"Evaluation/data/V69.fasta") # V6-9
source("config.R") # Link the configuration file with default program parameters and path to the data
# -*- coding: utf-8 -*-
#
#MetAmp - a software application for meta-amplicon data analysis.
#Written by Ilya Y. Zhbannikov, Feb 2, 2014.
#
# Setting the work directory:
#==============Source files
source("src/methods-Denoising.R")
source("src/methods-Merge.R")
source("src/methods-Convert.R")
source("src/methods-Clust.R")
source("src/methods-Dist.R")
source("src/methods-Stat.R")
source("src/methods-Affine.R")
source("src/methods-Misc.R")
source("src/methods-Triangulation.R")
# Libraries with functions used in MetAmp
library(seqinr) # For manupulations with sequences
library(CDHITConverter) # For translating the CD-HIT's output files into machine-readable format
library(BLASTParser) # Parses BLAST output file
library(tripack) # For triangulation
library(RANN) # For kd-tree
library(fpc) # For clustering using the DBSCAN algorithm
library(amap)
library(doMC) # For parallel clustering
library(foreach) # For parallel clustering
library(ShortRead) # For manipulations with sequences
#----Create "analysis" directory------#
analysis_path <- paste(dir_path, analysis_dir,sep='')
system(paste("mkdir", analysis_path))
# All information is recorded in log.txt:
logfile <- paste(analysis_path, '/', "log.txt", sep='')
writeMessage(paste("Analysis path: ", analysis_path), logfile, F)
writeMessage("Starting analysis...", logfile, T)
#----Data denoising------#
#work_libs <- matrix(nrow=length(libs), ncol=3)
#for (i in seq(libs)) {
#  if (denoise_data == T) {
#    ans <- denoise(infiles=libs[i],
#                   libtype="454",
#                   outprefix=paste(analysis_dir, '/', "work_lib", i, sep=''))
#  } else {
#    # Checking if provided files are in Fasta format:
#    #ans <- tryCatch(readFastq(paste(analysis_dir, '/', "work_lib", i, ".fasta", sep='')), error=function(e) {e <- -1}, finally=function(ans) {ans <--1})
#
#    #if (ans == -1) {
#    # If so, we just directly copy them into analysis directory:
#    system(paste("cp", libs[i], paste(analysis_dir, '/', "work_lib", i, ".fasta", sep='')))
#    #} else {
#    #  # Otherwise, rase an error and stop:
#    #  print("Error! Provided sequences not in Fasta format!")
#    #}
#  }
#  work_libs[i,1] <- paste(analysis_dir, '/', "work_lib", i, ".fasta", sep='')
#  work_libs[i,2] <- libs[i]
#
#  # Remove chimeras
#  removeChimeras(infile_reads=work_libs[i,1], infile_uchime=paste(work_libs[i,1],".uchime",sep=''))
#
#}
#
#
#
#print(paste("Running clustering application", cluster_app, "...") )
#registerDoMC(length(work_libs)) # Register cores and run in parallel
#foreach(i=1:dim(work_libs)[1]) %dopar%
#  cluster(analysis_dir, default_pref, work_libs[i,1])
work_libs <- matrix(nrow=length(libs), ncol=3)
for (i in seq(libs)) {
work_libs[i,1] <- libs[i] #paste(analysis_dir, '/', "work_lib", i, ".fasta", sep='')
work_libs[i,2] <- libs[i]
}
print(paste("Running clustering application", cluster_app, "...") )
for(i in 1:dim(work_libs)[1]) {
work_libs[i,1] <- cluster2(analysis_dir, default_pref, work_libs[i,1])
}
#==============Read distance matrices==================#
writeMessage("Distance matrix for 16S gene sequences...", logfile, T)
score16S <- generate_distance_matrix16S(ref16S)
writeMessage("Done!", logfile, T)
writeMessage("Distance matrix for guide and amplicon reads...", logfile, T)
scoresV <- readDistanceMatrices(work_libs) # This function reads pre-computed distance matrices sequentially
writeMessage("Done!", logfile, T)
#================Compute MDS for 16S=============#
writeMessage("Computing MDS for 16S...", logfile, T)
mds16S <- read16S(ref16S, score16S ) # Here we are computing a multi-dimensional scaling for 16S gene sequences.
num_ref_points <- dim(mds16S)[1] # Number of reference points.
writeMessage("Done!", logfile, T)
#=============Normalize amplicons=============#
# This is the core of the meta-amplicon analysis algorithm.
writeMessage("Normalization...", logfile, T)
summary_matrix <- normalizeAmplicons(scoresV, mds16S, num_ref_points)
writeMessage("Done!", logfile, T)
#=============Perform final clustering with DBSCAN================#
# Also is an important stage. We have to cluster everything after guide sequences carried back the empirical amplicons.
writeMessage("clustering with DBSCAN...", logfile, T)
tmp_clusters <- clusterDBSCAN(rownames(score16S), summary_matrix,scoresV)
writeMessage("Done!", logfile, T)
tmp_clusters
scoresV[[2]]
# -*- coding: utf-8 -*-
#
# MetAmp - a software application for meta-amplicon data analysis.
# Written by Ilya Y. Zhbannikov, Feb 2, 2014
# Updated on Sept 7, 2014
# Usage:
# Set the working directory by changing the "dir_path" variable
# Under R-environmentL $>source("main.R")
#
# Set the work directory:
dir_path <- "~/Projects/metamp/" # Working directory where all analysis data will be stored
setwd(dir_path)
# Provide your data here (can be raw or preprocessed libs, program assumes one file for each region):
#libs <- c("Evaluation/data/Dataset1/SRR053817_V1_V3/SRR053817_3.fastq", # V1-3
#          "Evaluation/data/Dataset1/SRR053818_V3_V5/SRR053818_3.fastq", # V3-5
#          "Evaluation/data/Dataset1/SRR053819_V6_V9/SRR053819_3.fastq") # V6-9
libs <- c("Evaluation/data/staggered/SRR072221_forward.fastq", # V1-3
"Evaluation/data/staggered/SRR072237_forward.fastq", # V3-5
"Evaluation/data/staggered/SRR072236_forward.fastq") # V6-9
# Reference sequences:
#ref16S <- "data/LTP/LTP-10271.fasta"
#refs <- c("data/LTP/LTP-10271_V13.fasta", # V1-3
#          "data/LTP/LTP-10271_V35.fasta", # V3-5
#          "data/LTP/LTP-10271_V69.fasta") # V6-9
ref16S <- "Evaluation/data/16S.fasta"
refs <- c("Evaluation/data/V13.fasta", # V1-3
"Evaluation/data/V35.fasta", # V3-5
"Evaluation/data/V69.fasta") # V6-9
source("config.R") # Link the configuration file with default program parameters and path to the data
# -*- coding: utf-8 -*-
#
#MetAmp - a software application for meta-amplicon data analysis.
#Written by Ilya Y. Zhbannikov, Feb 2, 2014.
#
# Setting the work directory:
#==============Source files
source("src/methods-Denoising.R")
source("src/methods-Merge.R")
source("src/methods-Convert.R")
source("src/methods-Clust.R")
source("src/methods-Dist.R")
source("src/methods-Stat.R")
source("src/methods-Affine.R")
source("src/methods-Misc.R")
source("src/methods-Triangulation.R")
# Libraries with functions used in MetAmp
library(seqinr) # For manupulations with sequences
library(CDHITConverter) # For translating the CD-HIT's output files into machine-readable format
library(BLASTParser) # Parses BLAST output file
library(tripack) # For triangulation
library(RANN) # For kd-tree
library(fpc) # For clustering using the DBSCAN algorithm
library(amap)
library(doMC) # For parallel clustering
library(foreach) # For parallel clustering
library(ShortRead) # For manipulations with sequences
#----Create "analysis" directory------#
analysis_path <- paste(dir_path, analysis_dir,sep='')
system(paste("mkdir", analysis_path))
# All information is recorded in log.txt:
logfile <- paste(analysis_path, '/', "log.txt", sep='')
writeMessage(paste("Analysis path: ", analysis_path), logfile, F)
writeMessage("Starting analysis...", logfile, T)
#----Data denoising------#
#work_libs <- matrix(nrow=length(libs), ncol=3)
#for (i in seq(libs)) {
#  if (denoise_data == T) {
#    ans <- denoise(infiles=libs[i],
#                   libtype="454",
#                   outprefix=paste(analysis_dir, '/', "work_lib", i, sep=''))
#  } else {
#    # Checking if provided files are in Fasta format:
#    #ans <- tryCatch(readFastq(paste(analysis_dir, '/', "work_lib", i, ".fasta", sep='')), error=function(e) {e <- -1}, finally=function(ans) {ans <--1})
#
#    #if (ans == -1) {
#    # If so, we just directly copy them into analysis directory:
#    system(paste("cp", libs[i], paste(analysis_dir, '/', "work_lib", i, ".fasta", sep='')))
#    #} else {
#    #  # Otherwise, rase an error and stop:
#    #  print("Error! Provided sequences not in Fasta format!")
#    #}
#  }
#  work_libs[i,1] <- paste(analysis_dir, '/', "work_lib", i, ".fasta", sep='')
#  work_libs[i,2] <- libs[i]
#
#  # Remove chimeras
#  removeChimeras(infile_reads=work_libs[i,1], infile_uchime=paste(work_libs[i,1],".uchime",sep=''))
#
#}
#
#
#
#print(paste("Running clustering application", cluster_app, "...") )
#registerDoMC(length(work_libs)) # Register cores and run in parallel
#foreach(i=1:dim(work_libs)[1]) %dopar%
#  cluster(analysis_dir, default_pref, work_libs[i,1])
work_libs <- matrix(nrow=length(libs), ncol=3)
for (i in seq(libs)) {
work_libs[i,1] <- libs[i] #paste(analysis_dir, '/', "work_lib", i, ".fasta", sep='')
work_libs[i,2] <- libs[i]
}
print(paste("Running clustering application", cluster_app, "...") )
for(i in 1:dim(work_libs)[1]) {
work_libs[i,1] <- cluster2(analysis_dir, default_pref, work_libs[i,1])
}
#==============Read distance matrices==================#
writeMessage("Distance matrix for 16S gene sequences...", logfile, T)
score16S <- generate_distance_matrix16S(ref16S)
writeMessage("Done!", logfile, T)
writeMessage("Distance matrix for guide and amplicon reads...", logfile, T)
scoresV <- readDistanceMatrices(work_libs) # This function reads pre-computed distance matrices sequentially
writeMessage("Done!", logfile, T)
#================Compute MDS for 16S=============#
writeMessage("Computing MDS for 16S...", logfile, T)
mds16S <- read16S(ref16S, score16S ) # Here we are computing a multi-dimensional scaling for 16S gene sequences.
num_ref_points <- dim(mds16S)[1] # Number of reference points.
writeMessage("Done!", logfile, T)
#=============Normalize amplicons=============#
# This is the core of the meta-amplicon analysis algorithm.
writeMessage("Normalization...", logfile, T)
summary_matrix <- normalizeAmplicons(scoresV, mds16S, num_ref_points)
writeMessage("Done!", logfile, T)
#=============Perform final clustering with DBSCAN================#
# Also is an important stage. We have to cluster everything after guide sequences carried back the empirical amplicons.
writeMessage("clustering with DBSCAN...", logfile, T)
tmp_clusters <- clusterDBSCAN(rownames(score16S), summary_matrix,scoresV)
writeMessage("Done!", logfile, T)
consensus_reads <- vector(mode="list", length=0 )
# Consensus reads & clusters:
for (i in seq(num_ref_points-4)) {
consensus_reads[[rownames(summary_matrix)[i]]] <- rownames(summary_matrix)[i]
}
consensus_reads
tmp_clusters$cluster
rownames(summary_matrix)[which(tmp_clusters$cluster == 3)]
length(unique(tmp_clusters$cluster))
# This script provides some statistical computations for meta-amplicon analysis
assignClusters <- function(tmp_clusters, work_libs) {
#consensus_reads <- vector(mode="list", length=0 )
## Consensus reads & clusters:
#for (i in seq(num_ref_points-4)) {
#  consensus_reads[[rownames(summary_matrix)[i]]] <- rownames(summary_matrix)[i]
#}
#clustered_libs <- matrix("", nrow=length(work_libs),ncol=1)
#for (i in 1:dim(work_libs)[1]) {
#  clustered_libs[i,] <- paste(analysis_dir, "/", basename(work_libs[i]), '_', cluster_suff, ".clstr", sep='')
#}
#
## Statistical analysis:
##First, determine the number of clusters for each consensus sequences:
#for ( k in seq(dim(work_libs)[1]) ) {
#  clusters <- convert(clustered_libs[k,])
#  for (i in seq(length(rownames(summary_matrix)))) {
#    for (j in seq(length(clusters))) {
#      if (rownames(summary_matrix)[i] %in% clusters[[j]]) {
#        consensus_reads[[rownames(summary_matrix)[i]]] <- clusters[[j]]
#        break
#      }
#    }
#  }
#}
# Then assign each read to its cluster:
tmp_final_clusters <- vector(mode="list", length=length(unique(tmp_clusters$cluster)))
for (i in 1:length(unique(tmp_clusters$cluster))) {
#for (ii in 1:length(rownames(summary_matrix)[which(tmp_clusters$cluster == i)])) {
#if (is.null(consensus_reads[ rownames(summary_matrix)[which(tmp_clusters$cluster == i)][ii] ][[1]])==F) {
#  if (is.null(tmp_final_clusters[[i]])) {
#    tmp_final_clusters[[i]] <- consensus_reads[ rownames(summary_matrix)[which(tmp_clusters$cluster == i)][ii] ][[1]]
#  } else {
#    tmp_final_clusters[[i]] <- c(tmp_final_clusters[[i]],consensus_reads[ rownames(summary_matrix)[which(tmp_clusters$cluster == i)][ii] ][[1]])
#  }
#}
#}
tmp_final_clusters[[i]] <- rownames(summary_matrix)[which(tmp_clusters$cluster == i)]
}
tmp_final_clusters <- tmp_final_clusters[!sapply(tmp_final_clusters, is.null)]
tmp_final_clusters
# Calculating the mean number of points in each cluster:
#final_statistics = vector(mode="list", length=length(unique(tmp_clusters$cluster)))
#for (i in 1:length(tmp_final_clusters)) {
#  final_statistics[[i]] <- mean( length(which( (tmp_final_clusters[[i]] %in% rownames(norm_data_v13[[2]]) ) == T))+1, length(which( (tmp_final_clusters[[i]] %in% rownames(norm_data_v35[[2]]) ) == T))+1, length(which( (tmp_final_clusters[[i]] %in% rownames(norm_data_v69[[2]]) ) == T))+1 )
#}
#tf_array <- matrix(0, nrow=1, ncol=length(tmp_final_clusters))
#for (i in seq(length(tmp_final_clusters))) {
# tf_array[1,i] <- length(which((rownames(mds16S) %in% tmp_final_clusters[[i]])==T))
#}
#if (max(tf_array) <= 20) {
#  break
#}
}
computeLargeClusters <- function(ref_points_list, clusters) {
#Here we do not remove an OTU if it is a reference OTU
largeClusters <- list()
cnt <- 0
for (i in 1:length(clusters)) {
if ( (length(clusters[[i]]) >= 10) || (isReferenceOTU(ref_points_list, clusters[[i]]) == T) ) {
cnt <- cnt + 1
largeClusters[[cnt]] <- clusters[[i]]
}
}
largeClusters
}
#=============Assign the final OTUs================#
writeMessage("Assigning the final OTUs...", logfile, T)
OTUS <- assignClusters(tmp_clusters, work_libs)
writeMessage("Done!", logfile, T)
OTUS
get_ref_clusters <- function(references, clusters, mds_v) {
ref_clusters <- c()
for(ref in rownames(references)) {
for(item in names(clusters)) {
if(ref %in% clusters[[item]]) {
ref_clusters <- c(ref_clusters,item)
break
}
}
}
ref_clusters
}
remove_small_clusters <- function(references, clusters, t) {
# This function removes the clusters with size less than 't'
# Parameters:
# references - an array of reference clusters
# clusters - an array of clusters
# t - a threshold (the minimum number of reads that have to be in a cluster)
# Here we will not remove a small OTU if it is a reference OTU.
ans <- list()
for(name in names(clusters)) {
if(length(clusters[[name]]) >= t ) {
ans[[name]] <- clusters[[name]]
}
}
ans
}
# Call clustering program
cluster <- function(analysis_dir, default_pref, lib) {
system(paste(cluster_app, "-c", cit, "-i", lib, "-o", paste(analysis_dir, "/", basename(lib), '_', cluster_suff, sep=''), "-g", "1", "-M", '0') )
}
# Perform the final clustering with DBSCAN:
clusterDBSCAN <- function(refnames, summary_matrix,scoresV) {
eps <- quantile(dist(summary_matrix),probs=0.005)[[1]] #0.025)
#tmp_clusters <- dbscan(summary_matrix[sample.int(nrow(summary_matrix)),],MinPts=2,  eps=eps)
tmp_clusters <- dbscan(summary_matrix,MinPts=1,  eps=eps)
#tmp_clusters <- dbscan(summary_matrix[sample.int(nrow(summary_matrix)),],MinPts=1,  eps*0.01)
#flag <- F
#while(flag==F) {
#  # Clustering the points using DBSCAN method:
#  tmp_clusters <- dbscan(summary_matrix,MinPts=1,  eps=eps)
#
#  for (i in seq(1,length(tmp_clusters$cluster))) {
#    #if ( length(which(tmp_clusters$cluster == i)) > 10 ) {
#    if ( length(getRefOTUNum(refnames, rownames(summary_matrix)[which(tmp_clusters$cluster == i)] )) > 1) {
#      eps <- eps/10
#      if (eps < 0) {
#        break
#      }
#      flag = F
#      break
#    } else {
#      flag = T
#    }
#  }
#
#  if (flag == T) {
#    break
#  }
#}
tmp_clusters
}
isReferenceOTU <- function(ref_points, otu) {
# This function checks if the otu if a reference otu
# Another input parameter is 'ref_points' - i.e. a list of reference points
ans <- F
if ( length(which( (otu %in% ref_points) == T) != 0) ) {
ans <- T
}
ans
}
getRefOTUNum <- function(ref_points, otu) {
# This function computes number of reference points in given OTU
ans <- 0
if ( length(which( (otu %in% ref_points) == T)) != 0) {
ans <- which( (otu %in% ref_points) == T)
}
ans
}
# Call clustering program
cluster2 <- function(analysis_dir, default_pref, lib) {
# Denoising:
denoisedlib <- paste(analysis_dir, "/", basename(lib), "_denoised", sep='')
system(paste(usearch7, "-fastq_filter", lib,  "-fastaout", denoisedlib, "-fastq_truncqual 15 -fastq_trunclen 250"))
# Dereplication:
dreplib <- paste(denoisedlib, "_drep", sep='')
system(paste(usearch7, "-derep_fulllength", denoisedlib, "-output", dreplib, "-sizeout"))
# Pre-clustering:
preclustlib <- paste(dreplib, "_pre", sep='')
system(paste(usearch7, "-cluster_smallmem", dreplib, "-centroids", preclustlib, "-sizeout -id 0.99 -maxdiffs 1"))
# Sorting (remove singletons):
sortlib <- paste(preclustlib, "_sorted", sep='')
system(paste(usearch7, "-sortbysize", preclustlib, "-output", sortlib, "-minsize 2"))
# Clustering:
clusterlib <- paste(sortlib, "_clustered", sep='')
system(paste(usearch7, "-cluster_otus", sortlib, "-otus", clusterlib, "-minsize 2"))
# Filtering chimeric sequences:
nochimericlib <- paste(clusterlib, "_nochimeric", sep='')
system(paste(usearch7, "-uchime_ref", clusterlib, "-db", ref16S, "-strand plus -nonchimeras", nochimericlib))
#print(nochimericlib)
nochimericlib
}
writeMessage("clustering with DBSCAN...", logfile, T)
tmp_clusters <- clusterDBSCAN(rownames(score16S), summary_matrix,scoresV)
writeMessage("Done!", logfile, T)
#=============Assign the final OTUs================#
writeMessage("Assigning the final OTUs...", logfile, T)
OTUS <- assignClusters(tmp_clusters, work_libs)
writeMessage("Done!", logfile, T)
OTUS
tmp_final_clusters[!sapply(tmp_final_clusters, is.null)]
tmp_clusters
